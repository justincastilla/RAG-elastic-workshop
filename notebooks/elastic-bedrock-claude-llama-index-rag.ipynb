{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Anthropic Claude 3, Amazon Bedrock and Llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will show you how to use Elastic search, Amazon Bedrock, Anthropic Claude 3 and llama-index to build a Retrieval Augmented Generation (RAG) solution\n",
    "\n",
    "\n",
    "#### Use case\n",
    "\n",
    "To demonstrate the RAG capability, let's take the use case of an AI Assistant that can help answer questions from a personal document. \n",
    "\n",
    "\n",
    "#### Persona\n",
    "You are Bob,an Application Developer at Anycompany. Anycompany is experiencing an overwhelming number of customer queries. Anycompany has built a secure and performant conversational AI Assistant to answer frequently asked questions. Now Anycompany wants this conversational AI assistant to be able to answer questions are which are specific to the company. \n",
    "\n",
    "In this workshop, you will build a context aware conversational AI Assistant for Anycompany \n",
    "\n",
    "#### Implementation\n",
    "To fulfill this use case, in this notebook we will show how to create a RAG Application to answer questions from business data. We will use  Elasticsearch, Anthropic Claude 3 Sonnet Foundation model, Amazon Bedrock and llama-index. \n",
    "We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration) for a free trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "To run this notebook you would need to install dependencies - llama-index and llama-index-llms-bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index --force-reinstall --quiet\n",
    "%pip install llama-index-llms-bedrock --force-reinstall --quiet\n",
    "%pip install llama-index-embeddings-bedrock --force-reinstall --quiet\n",
    "%pip install llama-index-vector-stores-elasticsearch --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Restart\n",
    "\n",
    "Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initiate Bedrock Runtime through llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "\n",
    "llm = Bedrock(\n",
    "   model=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "\n",
    "We'll use the Cloud ID to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to [Cloud ID](https://cloud.elastic.co/deployments) and select your deployment.\n",
    "\n",
    "We will use ElasticsearchStore to connect to our elastic cloud deployment. This would help create and index data easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "\n",
    "cloud_id = getpass(\"Elastic deployment Cloud ID: \")\n",
    "cloud_username = \"elastic\"\n",
    "cloud_password = getpass(\"Elastic deployment Password: \")\n",
    "index_name= \"new-index-1\"\n",
    "\n",
    "es = ElasticsearchStore(\n",
    "    index_name=index_name,\n",
    "    es_cloud_id=cloud_id, # found within the deployment page\n",
    "    es_user=\"elastic\",\n",
    "    es_password=cloud_password # provided when creating deployment. Alternatively can reset password.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ingestion Pipeline\n",
    "Create Ingestion pipeline to load the file, create embeddings and load it into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import json, os\n",
    "\n",
    "embeddingmodelId = 'amazon.titan-embed-text-v1' # change this to use a different embedding model\n",
    "\n",
    "model = BedrockEmbedding(model=embeddingmodelId)\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[SentenceSplitter(chunk_size=350, chunk_overlap=50),model,],\n",
    "        vector_store=es\n",
    "    )\n",
    "\n",
    "def get_documents_from_file(file):\n",
    "   \"\"\"Reads a json file and returns list of Documents\"\"\"\n",
    "\n",
    "   with open(file=file, mode='rt') as f:\n",
    "       conversations_dict = json.loads(f.read())\n",
    "      \n",
    "   # Build Document objects using fields of interest.\n",
    "   documents = [Document(text=item['conversation'],\n",
    "                         metadata={\"conversation_id\": item['conversation_id']})\n",
    "                for\n",
    "                item in conversations_dict]\n",
    "   return documents\n",
    "\n",
    "TMP_DIR = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'media')\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=TMP_DIR, required_exts=[\".json\"]\n",
    ")\n",
    "\n",
    "documents = reader.load_data()\n",
    "\n",
    "pipeline.run(documents=documents)\n",
    "print(\".....Done running pipeline.....\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Invocation and Response Generation\n",
    "\n",
    "Now that we have the passages stored in Elasticsearch and LLM is initialized, we can now ask a question to get the relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, QueryBundle, Response, Settings\n",
    "\n",
    "Settings.embed_model= model\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(es)\n",
    "query_engine = index.as_query_engine(llm, similarity_top_k=10)\n",
    "\n",
    "query=\"Give me summary of water related issues\"\n",
    "bundle = QueryBundle(query, embedding=Settings.embed_model.get_query_embedding(query))\n",
    "result = query_engine.query(bundle)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Elasticsearch Index\n",
    "\n",
    "Delete the Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(cloud_id=cloud_id, basic_auth=(cloud_username, cloud_password))\n",
    "es.options(ignore_status=[400,404]).indices.delete(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have now experimented with using `llama-index` SDK to get an exposure to Anthropic Claude 3 and Amazon Bedrock. Using llama-index you have generated an email responding to a customer due to their negative feedback.\n",
    "\n",
    "### Take aways\n",
    "- Adapt this notebook to experiment with different Claude 3 models available through Amazon Bedrock. \n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
